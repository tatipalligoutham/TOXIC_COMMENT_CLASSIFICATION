{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicCommentClassificationFinal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-1dmwfrcfCi"
      },
      "source": [
        "# <A href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\">Toxic Comment Classification</a>\n",
        "Given dataset of wikipedia comments, we are supposed to build a model that can predict the probability that the comments falls under the categories of toxicity (6 categories in this case)\n",
        "\n",
        "This is a supervised learning problem as we are given a dataset and the also if it falls in the toxic category (any one of the classes, or multiple classes)\n",
        "\n",
        "This is a classification problem as we are classifying the dataset into classes. Also, we are supposed to predict the probability that it falls under each classes, so this is also a regression problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW3nMXlDd2FB"
      },
      "source": [
        "We use three algorithms to train our model\n",
        "* Logistic Regression\n",
        "* Naive Bayes\n",
        "* Support Vector Machine\n",
        "\n",
        "We are given a train set, and a separate test set. So, we test out model against the given test set. We also divide the training set into 2 sets: train set and cross validation set so that we can tune the hyperparameters and choose the one which gives the maximum accuracy for the cross validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUFJSJhnfoU2"
      },
      "source": [
        "#import all the libraries\n",
        "from typing import *\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import re\n",
        "from copy import deepcopy\n",
        "from joblib import dump, load"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtvjylFyvmmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bdd4bcb-3bd3-4619-cbae-24b8f2294273"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqq5BBUne2as"
      },
      "source": [
        "# Exploring the dataset\n",
        "We now load the dataset and do some preliminary exploration of the data. This helps us better understand the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "618mjhaUewD3",
        "outputId": "337d1f04-5e38-4f8a-a9c7-5ed37fee692d"
      },
      "source": [
        "#Download the dataset\n",
        "!gdown --id 10N4pLNsHD69tv5DbJ-cji6s742wVXelb\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('jigsaw-toxic-comment-classification-challenge.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()\n",
        "with ZipFile('sample_submission.csv.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()\n",
        "with ZipFile('test.csv.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()\n",
        "  \n",
        "with ZipFile('test_labels.csv.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()\n",
        "\n",
        "with ZipFile('train.csv.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()\n",
        "!rm sample_submission.csv.zip test.csv.zip test_labels.csv.zip train.csv.zip jigsaw-toxic-comment-classification-challenge.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10N4pLNsHD69tv5DbJ-cji6s742wVXelb\n",
            "To: /content/jigsaw-toxic-comment-classification-challenge.zip\n",
            "55.2MB [00:00, 133MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO-FOTzYcI51"
      },
      "source": [
        "#load into dataframe\n",
        "sample_sub_df = pd.read_csv('sample_submission.csv', delimiter=',')\n",
        "test_df = pd.read_csv('test.csv', delimiter=',')\n",
        "test_label_df = pd.read_csv('test_labels.csv', delimiter=',')\n",
        "train_df = pd.read_csv('train.csv', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "5wRk6Q3MgP99",
        "outputId": "b7cb0130-3738-4d11-9d39-32e4ca06dcc4"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUyS4Y-rggpN"
      },
      "source": [
        "There are 6 categories of toxicity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNWzy0VFgbOv",
        "outputId": "3e533afa-64aa-4f4d-d99c-dc12e7c5ecf9"
      },
      "source": [
        "train_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 159571 entries, 0 to 159570\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count   Dtype \n",
            "---  ------         --------------   ----- \n",
            " 0   id             159571 non-null  object\n",
            " 1   comment_text   159571 non-null  object\n",
            " 2   toxic          159571 non-null  int64 \n",
            " 3   severe_toxic   159571 non-null  int64 \n",
            " 4   obscene        159571 non-null  int64 \n",
            " 5   threat         159571 non-null  int64 \n",
            " 6   insult         159571 non-null  int64 \n",
            " 7   identity_hate  159571 non-null  int64 \n",
            "dtypes: int64(6), object(2)\n",
            "memory usage: 9.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV1GJVI7g4rd"
      },
      "source": [
        "There are 159571 rows and no null values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1TloNh6g3Xc",
        "outputId": "d7379c98-1a53-4055-a75b-1bf41a97a177"
      },
      "source": [
        "#checking if there is any imbalance in the dataset\n",
        "cols = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
        "print(\"unique values\")\n",
        "for col in cols:\n",
        "  print(col)\n",
        "  print(train_df[col].value_counts())\n",
        "  print(\"*\"*29)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique values\n",
            "toxic\n",
            "0    144277\n",
            "1     15294\n",
            "Name: toxic, dtype: int64\n",
            "*****************************\n",
            "severe_toxic\n",
            "0    157976\n",
            "1      1595\n",
            "Name: severe_toxic, dtype: int64\n",
            "*****************************\n",
            "obscene\n",
            "0    151122\n",
            "1      8449\n",
            "Name: obscene, dtype: int64\n",
            "*****************************\n",
            "threat\n",
            "0    159093\n",
            "1       478\n",
            "Name: threat, dtype: int64\n",
            "*****************************\n",
            "insult\n",
            "0    151694\n",
            "1      7877\n",
            "Name: insult, dtype: int64\n",
            "*****************************\n",
            "identity_hate\n",
            "0    158166\n",
            "1      1405\n",
            "Name: identity_hate, dtype: int64\n",
            "*****************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKqocgEHhGls"
      },
      "source": [
        "* For each category, there are two classes: 0 (the comment is not toxic), 1 (the comment is toxic and the toxicity falls in this category)\n",
        "* The dataset is unbalanced, ie, most of the comments falls have label 0 ,meaning not toxic. This makes sense, as most of the comments in the social media is positive except in certain cases (in case of conflicts, ...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz1-AwXhhFhG",
        "outputId": "7d15e624-4757-4766-89ce-e05bb223a76f"
      },
      "source": [
        "#check if \"id\" has any info about the comment, or is just a row number\n",
        "rows = [i for i in range(train_df.shape[0])]\n",
        "sum(train_df[\"id\"]==rows)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnSapn1biSdM"
      },
      "source": [
        "So, the \"id\" column is just the row number and won't help in training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGdCo7YdiYD8",
        "outputId": "6ba0c428-bd23-47d1-fac1-b603bdf5b5d5"
      },
      "source": [
        "#remove stop words\n",
        "word_count = {}\n",
        "for sentence in train_df[\"comment_text\"]:\n",
        "  unique = set(sentence.split())\n",
        "  for word in unique:\n",
        "    if word not in word_count:\n",
        "      word_count[word] = 1\n",
        "    else:\n",
        "      word_count[word] += 1\n",
        "print(len(word_count))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "532299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oqvi6AcjDGr"
      },
      "source": [
        "Without any preprocessing, there are 532299 unique words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXrQifHujCBN",
        "outputId": "3d632595-c6b3-4f83-80dd-5cd36d11a795"
      },
      "source": [
        "unique_char = set()\n",
        "links = []\n",
        "for row in train_df[\"comment_text\"]:\n",
        "  unique_char.update(set(row))\n",
        "  m = re.search(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)',row)\n",
        "  if m:\n",
        "    links.append(m)\n",
        "print(len(unique_char))\n",
        "print(unique_char)\n",
        "alphabets = set()\n",
        "for c in unique_char:\n",
        "  if c.isalpha():\n",
        "    alphabets.add(c.lower())\n",
        "print(len(alphabets))\n",
        "print(alphabets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2335\n",
            "{'这', '独', 'ぬ', 'у', 'ɑ', '宕', 'ˤ', 'ノ', 'ẓ', '真', 'Ι', '£', 'ү', '✍', '熱', 'ấ', 's', 'ܘ', '橘', 'ர', '╗', 'Ö', '決', '⅝', '═', 'ằ', '̺', '◄', 'ｎ', 'ψ', 'ஒ', '∙', '你', '±', '≤', 'њ', 'Ӝ', '所', 'ľ', '干', '愛', 'ξ', '@', 'Ċ', 'ɪ', '啥', 'ギ', '৳', 'と', '祖', '太', '九', 'd', '′', 'ṭ', '岡', '̉', 'Ќ', '𒁳', 'ロ', 'ʼ', '高', '”', '言', 'w', '認', 'y', 'ố', 'ظ', 'ॆ', 'У', '㊟', 'Į', 'ṁ', 'ܐ', '려', '乙', 'Э', '২', '孫', '師', 'で', 'ʀ', 'ভ', 'ǚ', 'ʋ', '☼', 'घ', '论', '∆', '括', 'ო', 'ὂ', 'ݭ', '里', '王', 'Ű', 'Å', 'ງ', '人', 'ة', '㎥', 'り', 'ʃ', '敏', 'ण', '鹿', '¥', 'ç', '╚', '郡', 'ン', '项', '原', 'ブ', 'Ṇ', 'だ', 'ῶ', '民', 'ǂ', 'N', 'ɞ', '関', '३', '波', 'ʷ', 'z', 'き', 'ˢ', '╫', '隨', 'X', '个', 'L', '˜', 'μ', '谷', '龙', '\\x94', 'Ō', '君', '漢', 'Ŧ', 'غ', 'ǔ', '位', '😉', '◔', 'ॉ', 'l', '҅', 'べ', 'Ȳ', '𐌰', '‘', '胜', 'א', 'イ', 'இ', 'ட', '谢', 'へ', '्', 'ь', '☢', '∫', 'ŏ', 'お', '⅜', 'پ', 'Ḹ', 'ʁ', '⋅', 'シ', '÷', 'Ω', 'う', '🙉', '西', 'ず', '我', '書', '승', 'ボ', 'נ', 'Ո', '\\u202b', '堵', 'h', '古', 'K', 'າ', 'ㄤ', '독', 'ູ', 'ñ', '⅓', 'Ų', 'U', 'ุ', 'अ', 'ń', 'Β', '要', '治', 'எ', '柱', '吗', 'Ц', '\\x95', 'ත', '衛', '町', 'ɕ', 'ッ', 'Ž', 'ు', 'Ø', '≽', '\\uf737', '薬', 'ゴ', 'ﾉ', 'ı', '版', 'ो', 'ク', '黄', 'ɟ', 'á', '»', 'Ỹ', '討', '頭', '☄', 'ļ', 'ৌ', '̯', '♀', '合', 'ü', 'ろ', '部', 'ⲧ', '̂', '界', 'パ', 'Ǎ', 'ṣ', '눈', '驚', 'ை', '物', '❞', 'ನ', 'ট', '✌', '过', 'Æ', 'ρ', '的', '서', '⅞', 'ⁿ', 'ų', 'ǐ', 'า', 'ໄ', '芦', 'ɾ', 'ि', '👍', 'ɓ', '¢', '☮', '•', 'ර', '☛', 'ɜ', '공', 'ʔ', '猫', 'झ', '⟲', '公', 'ன', '×', 'ר', 'Ḡ', 'ˑ', '頁', 'ɝ', 'Ш', '₭', 'മ', 'ّ', '└', 'か', ']', '´', '勞', 'ή', '駅', '謚', 'ↄ', 'Т', 'ḷ', '但', '村', '\\u2002', 'ù', 'メ', '美', '軍', '★', '│', '稿', '\\u2009', 'º', 'ű', '钾', 'ต', '강', '침', 'ð', '‒', '№', '5', 'ū', ')', 'ו', 'っ', '0', 'オ', 'Ő', 'ẽ', '絡', 'ї', 'ɿ', 'を', '占', '雞', '\\uf734', 'ष', '說', 'Π', 'ャ', 'ກ', '甲', 'ᴬ', '̩', 'ד', '³', '記', 'Ġ', 'ნ', '₳', 'ī', '➨', '‽', 'த', '찜', '謝', '번', 'ু', 'О', '嗎', 'ｨ', '¤', 'Ü', 'п', '기', '么', '̎', '➪', 'ュ', '訳', 'ݣ', 'Α', '承', 'ǘ', '7', '家', '他', 'グ', 'ט', '洲', 'ह', '：', '墓', '琉', '✫', '純', 'ع', 'ͧ', '卷', '傳', 'ỉ', '！', 'ὰ', '∇', '労', '-', 'る', '反', '历', '_', '內', '闘', 'œ', 'ີ', '劇', 'մ', '卻', '北', '年', 'ಾ', '院', '操', '政', '⅔', 'م', 'М', '迷', '͇', '手', '满', 'ἠ', 'س', 'も', 'ʾ', 'ʜ', 'ʰ', '鮮', '₣', '‐', '₤', '✤', '😏', 'ゃ', '８', '拉', 'г', 'ס', 'ร', '行', '💜', 'ໜ', '情', '校', '已', 'ḥ', 'ȋ', 'ㄏ', 'ὀ', '搏', 'С', '├', 'ĭ', 'ֶ', 'ち', '山', '½', 'Ъ', '最', 'ɔ', '칠', 'R', '®', 'خ', '֑', '吉', '청', 'Ř', '臼', '💬', 'ど', 'ぞ', '²', 'Ż', '冒', '姻', 'ݟ', 'ὶ', 'י', 'ق', 'ペ', 'Σ', 'ᛟ', 'ா', 'െ', 'Е', '∑', '世', 'ˡ', '排', '┐', '대', '桜', 'ÿ', '初', '₮', '啼', 'ṗ', '△', 'К', '這', '喷', 'ɛ', 'е', '缎', '在', '海', 'ɣ', 'Υ', 'ै', '\\u202c', '是', '҉', 'Ó', 'آ', 'σ', '😂', 'ְ', 'თ', 'Ľ', 'ʡ', 'ё', 'b', '╟', '❤', '醫', '倒', '葦', 'Ψ', '故', 'ー', 'Ĵ', '\\uf03d', '🎤', 'Ë', 'Ŀ', 'ய', '۸', '♔', 'は', '共', 'ິ', '梁', 'Л', 'ק', '編', 'Ù', '活', '›', 'み', 'ُ', '蘭', '京', '御', 'í', '熊', '♩', 'ມ', 'щ', 'ば', '\\u200e', 'Ķ', '야', '滬', 'в', '⁽', 'כ', 'ែ', 'Ξ', '站', '✋', '白', '호', 'ṃ', '統', 'ｃ', '改', 'Ᵽ', '後', '出', 'セ', 'ʙ', 'ل', '¿', 'ґ', 'Ş', '◥', '恵', '投', '✗', 'ݜ', '✈', '袁', '咨', '視', 'ي', 'ং', '≼', '而', '\\uf738', 'ệ', '名', 'ت', '慕', '終', '屋', '范', '₧', '१', 'ザ', '益', 'κ', '⁄', '❦', '𐌲', 'ℓ', '☑', '다', 'â', 'プ', '௹', 'Ú', 'П', 'ɬ', '特', '╦', '网', 'Ϸ', 'ι', '方', '\\uf739', '᧾', 'ຸ', 'ḍ', 'ふ', 'ã', 'じ', '朝', 'ڈ', 'ܣ', 'ц', 'こ', '講', '✭', '係', '빠', 'ე', '이', '早', 'ò', '개', 'ἔ', '➥', 'ˈ', 'Ň', 'ȳ', 'շ', '》', 'ｱ', '商', 'Ϟ', '主', 'Î', 'Ã', '마', 'ນ', 'ｋ', '♝', 'ῃ', '．', '⇝', '鳴', 'ἡ', '᾽', '用', '柜', 'थ', '℥', '맛', 'ɫ', '副', '©', 'ย', '詠', '０', 'а', 'ા', '航', '✎', '專', 'ķ', '蛋', '߷', 'リ', '臧', 'ே', '１', 'ə', 'ล', '林', 'ě', 'ແ', 'ぎ', 'ひ', 'ღ', '・', '生', 'け', 'ਰ', 'Р', '☸', 'ʛ', 'ದ', '━', 'ג', 'ℱ', 'ख', '表', '东', 'औ', '済', 'ъ', 'દ', '織', '⊝', '粵', '川', '͍', '\\u2060', '來', '《', 'ʎ', '文', '録', '為', 'ẫ', 'ڜ', '都', '球', '被', 'ᚹ', 'ご', 'ㄧ', '͖', '✄', 'ὺ', '₰', 'ὄ', '句', 'រ', '石', '意', '外', 'ड', 'ς', 'Ṣ', 'ε', 'そ', '飞', 'ж', '♣', 'я', 'ム', 'ث', 'ז', '°', '!', '什', '✿', 'Ď', '≠', '緣', '┏', 'అ', 'Ƭ', '짐', '断', 'ᴷ', '清', 'ο', 'چ', 'ワ', '組', 'ソ', 'Ϻ', '¦', 'Ј', '笑', '撰', 'ë', '己', '猛', '่', 'ݓ', 'Ŝ', 'Ђ', 'Έ', '의', '🍌', '℠', 'フ', 'ứ', '/', '調', '器', 'и', 'χ', 'Ħ', 'Ī', '४', 'よ', 'ਨ', 'ˀ', 'ṛ', '憲', 'υ', '\\uf735', '|', 'コ', '\\xa0', '☿', 'ṯ', 'വ', 'ج', '͚', 'c', '瑚', 'Ε', '準', 'D', '집', '動', '銀', '\\x9d', '洛', '獨', 'ف', 'ђ', 'ز', '好', 'ί', 'ョ', 'ả', '包', '法', 'ș', '3', 'ɭ', 'ほ', 'ω', '容', '\"', '⁰', 'Ŭ', 'Љ', 'ċ', 'Õ', '🎄', 'ŗ', 'ŕ', '∧', 'Ä', '丈', '고', 'ɰ', '基', '″', '定', 't', 'Ἐ', '`', 'ً', 'Đ', '\\u202a', '若', '継', '番', 'অ', '฿', '∅', '盒', 'ɲ', 'ì', 'ą', 'ح', '風', 'ů', 'Ж', '焉', 'ヘ', '了', '➜', 'ᵃ', 'ẹ', 'Ģ', 'è', 'ັ', '砂', 'ி', '字', 'ѕ', 'ģ', 'ʌ', 'Ì', 'Ѕ', '혈', 'ǜ', '賜', '8', 'Ĉ', 'ł', 'ɻ', '♂', 'ï', '\\u2004', '就', '译', '६', '☾', '演', 'ු', 'ν', '造', 'し', 'ក', 'ɒ', 'ृ', '志', 'à', 'ʐ', 'ǣ', 'ô', 'Λ', 'Х', 'ɚ', '\\uf6fc', '陈', 'ợ', '格', '☤', 'ａ', '於', '黃', '‹', 'Œ', '\\uf731', '酸', 'ͮ', '筆', '抗', 'ｷ', '┘', '芜', 'ː', 'Ŏ', 'ה', 'す', '†', 'ⲟ', '珊', 'が', 'ᵮ', '雪', 'ر', 'ắ', '০', 'Ճ', '┓', 'ゅ', '구', '期', 'q', 'ǽ', '█', '=', '即', '訣', '■', '指', '屌', 'Γ', 'ύ', 'び', 'ō', 'Ϝ', '體', 'వ', 'त', 'Ч', '✽', '<', 'ч', 'ɡ', 'Ǒ', '巾', 'ذ', 'Ǣ', '₥', 'ヒ', 'ͩ', '€', '~', '∂', 'ý', '姓', '\\u200b', '넘', 'ņ', 'ň', '비', '火', '語', 'ē', '和', 'স', '肥', 'Њ', '連', '牛', 'あ', '̿', 'ก', 'ὲ', '≥', '→', 'つ', 'ॐ', 'ј', 'ს', 'Ǔ', '臭', '>', 'ż', 'ф', 'અ', 'ੁ', 'ä', '😄', 'џ', '🏼', '6', 'キ', '소', 'Ͱ', '濤', '天', 'ত', '督', '♪', '儀', '伝', '∀', '迎', 'ª', 'વ', 'O', 'ǝ', 'ホ', 'V', 'ị', 'ʕ', '、', 'ṅ', 'Y', '⅛', '二', '濟', '있', 'प', '\\uf0b7', 'Ô', 'え', '僑', '代', '乐', '‿', 'ộ', '藩', 'ח', 'ল', 'ב', '坡', '̄', '਼', 'Ę', 'ヮ', '☎', 'թ', 'ś', 'ׁ', '⊂', '竜', 'ხ', 'Ρ', 'ஜ', '路', '틀', 'ć', 'দ', '台', 'ء', 'ொ', '长', 'ˠ', '┗', '☯', 'ｏ', '福', '☠', '⨹', '∗', '輝', '灣', 'র', 'ſ', 'ῆ', 'Ĭ', 'श', 'ǁ', '↨', '✘', 'ǌ', 'Џ', 'ի', '话', '浦', 'ਮ', 'Ÿ', 'ই', 'È', 'Μ', 'Ｔ', '下', '▾', '牝', 'ℍ', 'ண', '有', 'ه', 'o', '\\u2028', 'な', 'ţ', 'न', 'ὅ', 'Ý', '\\x97', 'Ю', 'ո', '₩', '\\u2003', '张', '翁', '普', 'ѧ', 'ɠ', '務', '⟨', 'ė', '客', '¡', 'ス', '内', 'ຕ', 'Ḍ', '̗', 'ℚ', 'ʂ', '沖', 'ハ', '百', '۩', 'ἀ', '۵', 'ナ', 'Ŵ', '੭', '竹', 'ᛇ', '민', 'Č', 'ê', 'W', 'ừ', '草', '因', '官', 'ം', '♚', '₡', 'ক', 'ヽ', '날', '₯', '§', '維', 'É', 'Φ', 'ಅ', '잡', 'צ', 'r', '仙', 'ġ', 'ુ', '்', 'Ḷ', 'p', 'ּ', 'ʟ', 'ŷ', '地', '輯', '場', 'Û', 'Ĩ', '土', 'Ο', 'द', '卋', '令', 'E', 'č', 'ț', 'А', '女', '爽', '現', '度', 'ɮ', 'ま', 'C', 'J', '立', '史', 'ல', 'Þ', '惑', 'ş', '万', 'ஆ', 'ⲱ', '梨', '순', '亞', 'ら', '華', 'Щ', 'फ', 'ທ', 'ụ', 'Ṛ', 'Я', 'Ί', '\\u200f', 'ǫ', '』', '↑', 'ά', '達', '⁓', 'ب', 'ʞ', '자', 'ւ', '未', '后', 'आ', '្', 'Ĕ', '可', '군', '︵', '少', '#', '帝', '列', '景', '୯', '\\x93', 'ょ', 'Ū', '记', '麗', 'ケ', 'й', '或', '蝶', '“', '५', 'ɐ', 'g', '薩', 'Ӣ', 'ይ', 'ę', 'Й', '¸', 'ʧ', 'ெ', '¾', '✉', '需', '،', '窣', 'ḟ', '時', 'ʿ', 'ක', '対', '井', '題', 'உ', '♨', '☏', 'ɯ', 'Η', '面', 'জ', '̹', 'ປ', '中', '┌', 'ᶏ', '\\x7f', 'و', '९', 'Ḥ', '⁞', '目', 'ɘ', 'ˌ', 'H', '₫', 'म', 'て', '첫', '对', ' ', '↓', 'ŵ', '求', 'ɩ', 'e', '◅', '○', 'ע', '😢', '헌', 'ｗ', 'ठ', 'ფ', '妈', '⁂', '—', 'ສ', '╩', '子', '母', 'α', '阿', 'แ', 'य', '™', '不', '←', 'ຄ', 'ץ', 'レ', '길', '起', 'ő', 'ニ', 'ط', 'ᛏ', '\\uf04a', 'ｍ', 'ш', '蝴', 'ʊ', '『', '\\x91', 'ビ', 'š', 'ῖ', 'ⲩ', \"'\", 'ন', '者', '｡', '英', '静', 'ச', 'أ', 'ἐ', '̈', '̪', 'õ', 'İ', 'р', '\\u200a', '登', '思', '̥', '̮', '血', '智', '河', 'ﾍ', 'ं', 'வ', 'ש', 'Շ', '神', '༆', 'ў', 'Ŕ', '…', 'ئ', '₵', '주', 'ት', '۞', '制', 'Ũ', '本', '体', 'ữ', '◦', 'อ', '？', 'Н', '健', 'ế', '龜', '八', '◯', 'F', 'ǃ', '‖', 'ﾃ', '☺', '\\n', '県', '心', 'や', '♠', '壹', '市', 'ვ', 'a', 'é', '^', 'ʈ', 'ن', 'Ł', '☓', '很', '國', '유', '·', 'Ή', 'Í', '南', '澄', 'Ņ', '¯', 'å', 'Ў', 'ễ', 'ඳ', '退', '\\uf701', 'ʍ', '紫', 'ž', '🍁', '命', 'ĵ', 'ज', '\\u200d', 'Ύ', 'ø', 'Ѓ', 'ݗ', '【', ';', '࿓', '擲', 'ʄ', '্', 'ŀ', '는', 'ប', '척', '永', '信', '服', 'ǎ', 'ө', '⁾', '既', 'ジ', 'ậ', '₴', '線', '1', 'ַ', '😀', 'ാ', 'Ļ', 'ℳ', '援', '製', '*', 'ざ', '翼', '今', 'ɹ', 'Δ', 'о', '加', '%', '通', '˺', '뉴', '₢', '״', 'ď', 'ﾟ', 'ʻ', 'þ', '實', 'ḹ', '縄', 'ດ', 'τ', 'Ţ', 'ĕ', '₨', '知', 'ṫ', '总', 'Ě', '便', '険', '\\uf6de', '😜', 'v', '蒙', 'ư', 'ǰ', 'Ǫ', '⊥', 'ɱ', 'ό', '▪', 'ὧ', '⁴', '☥', 'ɦ', 'ɥ', '〈', '島', 'भ', '圍', 'サ', 'ম', '♦', 'տ', 'B', 'য', 'ங', 'і', 'め', 'ल', '遯', '조', '⟩', '田', '∪', 'स', 'ವ', 'ֵ', '៛', 'Ė', '安', '藏', '선', '廣', '縣', 'х', 'ป', 'ɤ', '並', '科', 'ル', '偏', 'ು', 'Ǐ', '閩', '요', '術', '绍', '්', '𐌴', 'Ά', '新', 'ك', '✰', '̓', '경', 'ḻ', 'k', '省', '𐌿', '電', '羊', '輸', 'ћ', 'Á', '歡', '栗', '付', '変', '金', 'ˉ', '\\x92', '防', '✓', '朱', '\\u202f', 'ó', '2', 'î', '雲', 'ヴ', 'ặ', 'x', 'Ĥ', '☇', '只', 'І', '韦', 'ピ', '利', '↕', 'گ', '◕', 'க', 'ǖ', 'ն', '洪', '声', 'ր', '리', '品', 'エ', '⇄', '磨', '◁', '您', 'ా', '府', 'Ϛ', 'ァ', 'ͨ', '―', 'ʉ', 'ㄉ', '🗽', 'ؤ', '♥', '助', 'ട', '国', 'Q', 'Ṃ', '╢', '∴', 'Қ', '9', '武', '犬', 'i', 'Ν', '參', '做', 'Ζ', 'ی', '☞', 'ڰ', '़', '❝', 'ŭ', 'ब', ',', 'ヌ', 'ラ', 'ầ', 'к', '泛', 'カ', '兼', '룡', '?', 'ź', '疋', '받', '׀', 'ব', '東', 'Ώ', 'д', '̰', 'ℝ', 'ว', 'ὸ', 'ể', 'タ', '紅', '臺', 'ʨ', '青', 'Ґ', 'т', '⊗', 'Ї', '雖', '🙈', '渡', '陳', '桔', '長', '┃', 'ũ', 'Ͼ', '‡', '稱', '‑', '水', '¼', '聖', '湖', '疆', 'В', 'ん', '¶', 'ົ', '�', 'ग', 'ト', 'л', '江', 'ව', 'Ы', 'た', '十', '−', 'ა', 'ლ', 'ね', 'ú', '龱', '翻', 'Ĺ', '区', 'Ŷ', '涛', 'わ', 'ㅅ', '執', '６', 'হ', 'ܝ', '문', '一', '인', '편', '✐', 'Ṝ', '{', '牡', 'ລ', '次', '素', '兵', 'ア', 'ോ', 'Ð', 'ʢ', 'ோ', 'ಠ', '慧', '७', 'さ', 'Д', '感', 'ค', 'ҝ', '≡', 'ਖ', 'б', '聯', '̲', '班', 'Ź', '̫', '̃', 'ζ', '楊', 'ö', '່', 'Ṭ', '」', '̞', '아', '越', 'ィ', 'チ', '✒', '例', '안', 'ჷ', 'Ь', '☜', '✔', 'ʲ', '☣', 'ך', '\\uf730', '「', '역', '先', 'M', 'ğ', '粿', '+', '🙊', 'æ', '良', '۾', 'ｌ', '❉', 'қ', '‧', 'Â', '〈', '҈', 'ک', 'ۻ', 'ຈ', '郭', '汉', 'ա', 'ো', 'Ό', 'の', 'న', 'ṝ', 'Š', '¹', 'ⱷ', '訪', '－', 'ऋ', 'ǒ', 'Ť', '莱', '上', 'ῷ', '✆', '😃', 'с', 'ش', 'ウ', 'ќ', 'ѓ', 'ਜ', 'Ё', '⊙', 'モ', 'ᵏ', '近', '☻', 'ờ', '学', '（', '💩', 'f', 'ु', '到', '∈', '事', '4', '͑', 'ہ', 'उ', '𐌹', '\\xad', 'ﾞ', 'Ẽ', '₂', 'έ', '働', '⚇', 'い', 'ֲ', 'Ñ', 'ŋ', 'ᵀ', 'ā', 'λ', 'व', 'ָ', 'Ͳ', '四', '¬', '郎', 'є', 'ீ', '̆', 'į', 'ᾳ', '話', '☃', 'ŧ', '̜', '大', '寧', 'ƒ', 'ן', 'ǧ', '曹', 'ß', '☘', 'Z', '小', '劉', 'く', 'テ', 'ἴ', 'Ï', 'Ａ', '[', '☀', 'அ', 'ȗ', '折', '$', 'ㄷ', 'ぜ', '\\uf0a7', 'ا', '以', 'ม', 'љ', '馬', 'إ', 'φ', 'Ⅱ', '然', 'ŝ', 'ἰ', 'δ', 'ﬂ', '▫', '▶', 'u', 'ӧ', '烂', 'ɴ', 'ọ', 'ĺ', 'ન', 'P', '–', '日', 'ɸ', '吳', '⊕', 'ट', '͡', '√', '☽', 'ᴥ', 'ỏ', 'ʑ', 'Θ', 'സ', '도', 'β', 'פ', 'Ө', '步', 'Ћ', '\\ufeff', '页', '\\uf6d9', 'Ò', '辺', 'Ğ', 'ɧ', 'ᴀ', '보', 'ந', 'ে', 'ם', '颈', 'ĥ', 'د', 'Ŗ', 'ᵗ', '¨', '仮', 'ு', '়', 'ܪ', '捏', '┛', '使', 'ص', '楩', 'მ', 'ற', '岩', '✞', '(', 'ᾶ', 'ɺ', 'И', '̼', '佩', 'Ů', 'र', 'デ', 'ִ', '成', '貢', 'ພ', 'ँ', '義', 'ຼ', '船', '塩', 'ħ', '們', '☝', 'գ', '溪', 'À', 'პ', 'გ', 'Ⓣ', 'ा', '╝', '松', '́', 'ი', '撃', '論', '靜', '്', 'ỹ', 'н', '̴', '選', '油', 'რ', '↗', '／', 'ͤ', 'ώ', '\\\\', 'れ', '平', '͉', 'µ', '▎', 'ы', '&', '佐', '≈', '─', 'ㅂ', 'バ', '，', 'з', 'ٔ', 'ക', 'ழ', 'ɳ', '०', 'Κ', '照', '虞', '隻', '२', '«', 'Ç', '⚔', 'Ś', 'ദ', '泥', 'Ə', 'ப', '̭', 'ਾ', '瀾', 'ະ', 'Ĝ', '旭', 'ă', '지', 'ὼ', '。', 'Ⓐ', 'ी', 'ť', 'ɶ', '❖', 'յ', '☆', '১', 'ɖ', 'ɢ', '三', '學', '注', 'З', 'ơ', 'ው', 'η', '味', '会', '氏', '♑', '邈', '尻', 'Χ', '介', '║', '祝', '明', '📧', '„', '布', 'ř', '豆', '）', 'ວ', 'S', 'ள', 'Ć', 'θ', 'ṇ', '\\uf736', '͙', 'ɗ', '↔', 'マ', '}', 'ໂ', 'Ⅎ', 'ῼ', 'Ὀ', 'ம', 'n', 'ɵ', 'ض', '戸', 'ɨ', '⌊', 'מ', '\\uf733', '吕', ':', 'Ą', '庄', '置', 'ツ', '坊', 'Г', 'せ', 'া', '典', 'ʘ', '前', 'げ', 'ĝ', 'Є', '∞', '〉', 'ຣ', '同', 'ध', 'û', 'đ', '때', '☭', 'ῦ', 'ຊ', '’', 'ɽ', '\\u06dd', '卐', 'ἑ', '😅', 'ｳ', 'ự', 'ద', '守', 'ド', 'Ē', '۬', 'ਸ', 'ຫ', 'ອ', '\\u3000', '･', 'Ā', 'ʒ', 'ṙ', '◀', '८', '紙', '♬', '見', '野', 'Б', 'T', 'э', '也', 'ĩ', 'ى', 'Ń', '͓', '号', '老', 'Ă', 'े', 'ʇ', 'に', '快', '刀', '●', '攻', '七', '威', 'ю', '千', 'ര', '➔', 'ˁ', '₪', 'ݡ', '沪', 'ṧ', '柳', '😊', 'ू', '龍', 'च', '夜', 'м', '⇒', '影', 'ਫ', 'ל', 'ڵ', 'm', '止', '月', '自', '逆', '】', 'ֻ', '📞', '상', 'I', '別', '₦', '穣', '忍', '職', '⇔', '►', 'ポ', '๛', '😔', 'ᴸ', 'ຮ', 'Ф', 'क', '₠', 'ạ', 'ベ', '.', 'Τ', '写', 'j', '̠', 'ㄨ', 'ἱ', '▲', 'ڬ', '\\uf732', '顧', 'Ê', 'G', '♫', 'इ', 'ĉ', 'ת', '陸', '천', 'ʝ', 'γ', 'π', 'ử', 'A', '陽', '╔', 'ǀ', 'ガ', 'ʏ', 'ェ', 'ি', '\\x99'}\n",
            "1542\n",
            "{'这', '独', 'ぬ', 'у', 'ɑ', '宕', 'ˤ', 'ノ', 'ẓ', '真', 'ү', '熱', 'ấ', 's', 'ܘ', '橘', 'ர', '決', 'ằ', 'ｎ', 'ψ', 'ஒ', '你', 'њ', '所', 'ľ', '干', '愛', 'ξ', 'ɪ', '啥', 'ギ', 'と', '祖', '太', '九', 'd', 'ṭ', '岡', '𒁳', 'ロ', 'ʼ', '高', '言', 'w', '認', 'y', 'ố', 'ظ', 'ṁ', 'ܐ', '려', '乙', '孫', '師', 'で', 'ʀ', 'ভ', 'ǚ', 'ʋ', 'घ', '论', '括', 'ო', 'ὂ', 'ݭ', '里', '王', 'ງ', '人', 'ة', '敏', 'り', 'ʃ', 'ण', '鹿', 'ç', '郡', 'ン', '项', '原', 'ブ', 'だ', 'ῶ', '民', 'ǂ', 'ɞ', '関', '波', 'ʷ', 'z', 'き', 'ˢ', '隨', 'ӣ', '个', 'μ', '谷', '龙', '君', '漢', 'غ', 'ǔ', '位', 'l', 'べ', '𐌰', '胜', 'א', 'イ', 'இ', 'ட', '谢', 'へ', 'ь', 'ŏ', 'お', 'پ', 'ʁ', 'シ', 'う', '西', 'ず', '我', '書', '승', 'ボ', 'נ', '堵', 'h', '古', 'າ', 'ㄤ', '독', 'ñ', 'अ', 'ń', '要', '治', 'எ', '柱', '吗', 'ත', '衛', '町', 'ɕ', 'ッ', 'ゴ', '薬', 'ﾉ', 'ı', '版', 'ク', '黄', 'ɟ', 'á', 'ϻ', '討', '頭', 'ļ', '合', 'ü', 'ろ', '部', 'ⲧ', '界', 'パ', 'ṣ', '눈', '驚', '物', 'ನ', 'ট', '过', 'ρ', '的', '서', 'ⁿ', 'ų', 'ǐ', 'า', 'ໄ', '芦', 'ɾ', 'ɓ', 'ර', 'ɜ', '공', 'ʔ', '猫', 'झ', '公', 'ன', 'ר', 'ˑ', '頁', 'ɝ', 'മ', 'か', '勞', 'ή', '駅', '謚', 'ↄ', 'ḷ', '但', '村', 'ù', 'メ', '美', '軍', '稿', 'º', 'ű', '钾', 'ต', '강', '침', 'ð', 'ͼ', 'ū', 'ו', 'っ', 'オ', 'ẽ', '絡', 'ї', 'ɿ', 'を', '占', 'i̇', '雞', 'ष', '說', 'ャ', 'ກ', '甲', 'ᴬ', 'ד', '記', 'ნ', 'ī', 'த', '찜', '謝', '번', '嗎', 'ｨ', 'п', '么', '기', 'ュ', '訳', 'ݣ', '承', 'ǘ', '家', '他', 'グ', 'ט', '洲', 'ह', '墓', '琉', '純', 'ع', '卷', '傳', 'ỉ', 'ὰ', '労', 'る', '反', '历', '內', '闘', 'œ', '劇', 'մ', '卻', '北', '年', '院', '操', '政', 'م', '迷', '手', '满', 'ἠ', 'س', 'も', 'ʾ', 'ʜ', 'ʰ', '鮮', 'ϟ', 'ゃ', '拉', 'г', 'ס', 'ร', '行', 'ໜ', '情', '校', '已', 'ḥ', 'ȋ', 'ㄏ', 'ὀ', '搏', 'ḡ', 'ĭ', 'ち', '山', '最', 'ɔ', '칠', 'خ', '吉', '청', '臼', 'ど', 'ぞ', '冒', '姻', 'ݟ', 'ὶ', 'י', 'ق', 'ペ', 'ᛟ', '世', 'ˡ', '排', '대', '桜', 'ÿ', '初', '啼', 'ṗ', 'ｔ', '這', '喷', 'ɛ', 'е', '缎', '在', '海', 'ɣ', '是', 'آ', 'σ', 'თ', 'ʡ', 'ё', 'b', '醫', '倒', '葦', '故', 'ー', 'ய', 'は', '共', '梁', 'ק', '編', '活', 'み', '蘭', '京', '御', 'í', '熊', 'ມ', 'щ', 'ば', '야', '滬', 'в', 'כ', '站', '白', '호', 'ṃ', '統', 'ｃ', '改', 'セ', '後', '出', 'ʙ', 'ل', 'ґ', '恵', '投', 'ݜ', '袁', '咨', '視', 'ي', '而', 'ệ', '名', 'ت', '慕', '終', '屋', '范', 'ザ', '益', 'κ', '𐌲', 'ℓ', '다', 'â', 'プ', 'ɬ', '特', '网', 'ι', '方', 'ḍ', 'ふ', 'ã', 'じ', '朝', 'ڈ', 'ܣ', 'ϝ', 'ц', 'こ', '講', 'ͳ', '係', '빠', 'ე', '이', '早', 'ò', '개', 'ἔ', 'ˈ', 'շ', 'ȳ', 'ｱ', '商', '主', '마', 'ນ', 'ｋ', 'ῃ', '鳴', 'ἡ', '用', '柜', 'थ', '맛', 'ɫ', '副', 'ย', '詠', 'а', '蛋', '航', '專', 'ķ', 'リ', '臧', 'ə', 'ล', '林', 'ě', 'ແ', 'ぎ', 'ひ', 'ღ', '生', 'け', 'ਰ', 'ʛ', 'ದ', 'ג', 'ℱ', 'ख', '表', '东', 'ͱ', 'औ', '済', 'ъ', 'દ', '織', '粵', '川', '來', 'ʎ', '文', '録', '為', 'ẫ', 'ڜ', '都', '球', '被', 'ᚹ', 'ご', 'ㄧ', 'ὺ', 'ⅎ', 'ὄ', '句', 'រ', '石', '意', '外', 'ड', 'ς', 'ε', 'そ', '飞', 'ж', 'я', 'ム', 'ث', 'ז', '什', '緣', 'అ', '짐', '断', 'ᴷ', '清', 'ο', 'چ', 'ワ', '組', 'ソ', '笑', '撰', 'ë', '己', '猛', 'ݓ', '의', 'フ', 'ứ', '調', '器', 'и', 'χ', 'よ', 'ਨ', 'ˀ', 'ṛ', '憲', 'υ', 'コ', 'ṯ', 'വ', 'ج', 'c', '瑚', '準', '집', '動', '銀', '洛', '獨', 'ف', 'ђ', 'ز', '好', 'ί', 'ョ', 'ả', '包', '法', 'ș', 'ɭ', 'ほ', 'ω', '容', 'ċ', 'ŗ', 'ŕ', '丈', '고', 'ɰ', '基', '定', 't', '若', '継', '番', 'অ', '盒', 'ɲ', 'ì', 'ą', 'ح', '風', 'ů', '焉', 'ヘ', '了', 'ᵃ', 'ẹ', 'è', '砂', '字', 'ѕ', 'ģ', 'ʌ', '혈', 'ǜ', '賜', 'ł', 'ɻ', 'ï', '就', '译', '演', 'ν', '造', 'し', 'ក', 'ɒ', '志', 'à', 'ʐ', 'ǣ', 'ô', 'ɚ', '陈', 'ợ', '格', 'ａ', '於', '黃', '酸', '抗', '筆', 'ｷ', '芜', 'ː', 'ה', 'す', 'ⲟ', '珊', 'が', 'ᵮ', '雪', 'ر', 'ắ', 'ゅ', '구', '期', 'q', 'ǽ', '即', '訣', '指', '屌', 'ύ', 'び', 'ō', '體', 'వ', 'त', 'ч', 'ɡ', '巾', 'ذ', 'ヒ', 'ý', '姓', '넘', 'ņ', 'ň', '비', '火', '語', 'ē', '和', 'স', '肥', '連', '牛', 'あ', 'ก', 'ὲ', 'つ', 'ॐ', 'ј', 'ს', '臭', 'ż', 'ф', 'અ', 'ä', 'џ', 'キ', '소', '濤', '天', 'ত', '督', '儀', '伝', '迎', 'ª', 'વ', 'ǝ', 'ホ', 'ị', 'ʕ', 'ṅ', '二', '濟', '있', 'प', 'え', '僑', '代', '乐', 'ộ', '藩', 'ח', 'ল', 'ב', '坡', 'ヮ', 'թ', 'ś', '竜', 'ხ', 'ஜ', '路', '틀', 'ć', 'দ', '台', 'ء', '长', 'ˠ', 'ｏ', '福', '輝', '灣', 'র', 'ſ', 'ῆ', 'श', 'ǁ', 'ǌ', 'ի', '话', '浦', 'ਮ', 'ই', '下', '牝', 'ண', 'ℍ', '有', 'ه', 'o', 'な', 'ţ', 'न', 'ὅ', 'ո', '张', '翁', '普', 'ѧ', 'ɠ', '務', 'ė', '客', 'ス', '内', 'ຕ', 'ℚ', 'ʂ', '沖', 'ハ', '百', 'ἀ', 'ナ', '竹', 'ᛇ', '민', 'ê', 'ừ', '草', '因', '官', 'ক', 'ヽ', '날', '維', 'ಅ', '잡', 'צ', 'r', '仙', 'ġ', 'ῳ', 'p', 'ʟ', 'ŷ', '地', '輯', '場', '土', 'द', '卋', '令', 'č', 'ț', '女', '爽', '現', '度', 'ɮ', 'ま', '立', '史', 'ல', '惑', 'ş', '万', 'ஆ', 'ⲱ', '梨', '순', '亞', 'ら', '華', 'फ', 'ທ', 'ụ', 'ǫ', '達', 'ά', 'ب', 'ʞ', '자', 'ւ', '未', '后', 'आ', '可', '군', '少', '帝', '列', '景', 'ょ', '记', '麗', 'ケ', 'й', '或', '蝶', 'ɐ', 'g', '薩', 'ይ', 'ę', 'ʧ', '需', '窣', 'ḟ', '時', 'ʿ', 'ක', '対', '井', '題', 'உ', 'ɯ', '面', 'জ', 'ປ', '中', 'ᶏ', 'و', '目', 'ɘ', 'ˌ', 'म', '对', 'て', '첫', 'ŵ', '求', 'ɩ', 'e', 'ע', '헌', 'ｗ', 'ठ', 'ფ', '妈', 'ສ', '子', '母', 'α', '阿', 'แ', 'य', '不', 'ຄ', 'ץ', 'レ', '길', '起', 'ő', 'ニ', 'ط', 'ᛏ', 'ｍ', 'ш', '蝴', 'ʊ', 'ビ', 'š', 'ῖ', 'ⲩ', 'ন', '者', '英', '静', 'ச', 'أ', 'ἐ', 'õ', 'р', '登', '思', '血', '智', '河', 'ﾍ', 'வ', 'ש', '神', 'ў', 'ئ', '주', 'ት', '制', '本', '体', 'ữ', 'อ', '健', 'ế', '龜', '八', 'ǃ', 'ﾃ', '県', '心', 'や', '壹', '市', 'ვ', 'a', 'é', 'ʈ', 'ن', '很', '國', '유', '南', '澄', 'å', 'ễ', 'ඳ', '退', 'ʍ', '紫', 'ž', '命', 'ĵ', 'ज', 'ø', 'ݗ', '擲', 'ʄ', 'ŀ', '는', 'ប', '척', '永', '信', '服', 'ǎ', 'ө', '既', 'ジ', 'ậ', '線', 'ℳ', '援', '製', 'ざ', '翼', '今', 'ɹ', 'о', '加', '通', '뉴', 'ď', 'ﾟ', 'ʻ', 'þ', '實', 'ḹ', '縄', 'ດ', 'τ', 'ĕ', 'ṫ', '知', '总', '便', '険', 'v', '蒙', 'ư', 'ǰ', 'ɱ', 'ό', 'ϛ', 'ὧ', 'ɦ', 'ɥ', '島', 'भ', '圍', 'サ', 'ম', 'տ', 'য', 'ங', 'і', 'め', 'ल', '遯', '조', '田', 'स', 'ವ', '安', '藏', '선', '廣', '縣', 'х', 'ป', 'ɤ', '並', '科', 'ル', '偏', '閩', '요', '術', '绍', '𐌴', '新', 'ƭ', 'ك', '경', 'ḻ', 'k', '省', '𐌿', '電', '羊', '輸', 'ћ', '栗', '歡', '付', '変', '金', 'ˉ', '防', '朱', 'ó', 'î', '雲', 'ヴ', 'ặ', 'x', '只', 'ճ', '韦', 'ピ', '利', 'گ', 'க', 'ǖ', 'ն', '洪', '声', 'ր', '리', '品', 'エ', '磨', '您', '府', 'ァ', 'ʉ', 'ㄉ', 'ؤ', '助', 'ട', '国', '武', '犬', 'i', '做', '參', 'ӝ', 'ی', 'ڰ', 'ŭ', 'ब', 'ヌ', 'ラ', 'ầ', 'к', '泛', 'カ', '兼', '룡', 'ź', '疋', '받', 'ব', '東', 'д', 'ℝ', 'ว', 'ὸ', 'ể', 'タ', '紅', '臺', 'ʨ', '青', 'т', '雖', '渡', '陳', '桔', '長', 'ũ', '稱', '水', '聖', '湖', '疆', 'ん', 'ग', 'ト', 'л', '江', 'ව', 'た', '十', 'ა', 'ლ', 'ね', 'ú', '龱', '翻', '区', '涛', 'わ', 'ㅅ', '執', 'হ', 'ܝ', '문', '一', '인', '편', '牡', 'ລ', '次', '素', '兵', 'ア', 'ʢ', 'ಠ', '慧', 'さ', '感', 'ค', 'ҝ', 'ਖ', 'б', '聯', '班', 'ζ', '楊', 'ö', '아', '越', 'ィ', 'チ', '例', '안', 'ჷ', 'ʲ', 'ך', '역', '先', 'ğ', '粿', 'æ', '良', 'ｌ', 'қ', 'ۻ', 'ک', 'ຈ', '郭', '汉', 'ա', 'の', 'న', 'ṝ', 'ⱷ', '訪', 'ऋ', 'ǒ', '莱', '上', 'ῷ', 'с', 'ش', 'ウ', 'ќ', 'ѓ', 'ਜ', 'モ', 'ᵏ', '近', 'ờ', '学', 'f', '到', '事', 'ہ', 'उ', '𐌹', 'ﾞ', 'έ', '働', 'い', 'ŋ', 'ᵀ', 'ā', 'λ', 'व', '四', '郎', 'є', 'į', 'ᾳ', '話', 'ŧ', '大', '寧', 'ƒ', 'ן', 'ǧ', '曹', 'ß', '小', '劉', 'く', 'テ', 'ἴ', 'அ', 'ȗ', '折', 'ㄷ', 'ぜ', 'ا', '以', 'ม', 'љ', '馬', 'إ', 'φ', '然', 'ŝ', 'ἰ', 'δ', 'ﬂ', 'u', 'ӧ', '烂', 'ɴ', 'ọ', 'ĺ', 'ન', '日', 'ɸ', '吳', 'ट', 'ᴥ', 'ỏ', 'ʑ', 'സ', '도', 'β', 'פ', '步', '页', '辺', 'ɧ', 'ᴀ', '보', 'ந', 'ם', '颈', 'ĥ', 'د', 'ᵗ', '仮', 'ܪ', '捏', '使', 'ص', '楩', 'მ', 'ற', '岩', 'ᾶ', 'ɺ', '佩', 'र', 'デ', '成', '貢', 'ພ', '義', '船', '塩', 'ħ', '們', 'գ', '溪', 'პ', 'გ', '松', 'ი', '撃', '論', '靜', 'ỹ', 'н', '選', '油', 'რ', 'ώ', 'れ', '平', 'µ', 'ы', '佐', 'ㅂ', 'バ', 'з', 'ക', 'ழ', 'ᵽ', 'ɳ', '照', '虞', '隻', 'ദ', '泥', 'ப', '瀾', 'ະ', '旭', 'ă', '지', 'ὼ', 'ť', 'ɶ', 'յ', 'ɖ', 'ɢ', '三', '學', '注', 'ơ', 'ው', 'η', '味', '会', '氏', '邈', '尻', '介', '祝', '明', '布', 'ř', '豆', 'ວ', 'ள', 'θ', 'ṇ', 'ɗ', 'マ', 'ໂ', 'ம', 'n', 'ɵ', 'ض', '戸', 'ɨ', 'מ', '吕', '庄', '置', 'ツ', '坊', 'せ', '典', 'ʘ', '前', 'げ', 'ĝ', 'ຣ', '同', 'ध', 'û', 'đ', '때', 'ῦ', 'ຊ', 'ɽ', '卐', 'ἑ', 'ｳ', 'ự', 'ద', '守', 'ド', 'ਸ', 'ຫ', 'ອ', 'ʒ', 'ṙ', '紙', '見', '野', 'э', '也', 'ĩ', 'ى', '号', '老', 'ʇ', 'に', '刀', '快', '攻', '七', '威', 'ю', '千', 'ര', 'ˁ', 'ݡ', '沪', 'ṧ', '柳', '龍', 'च', '夜', 'м', '影', 'ਫ', 'ל', 'ڵ', 'm', '止', '月', '自', '逆', '상', '別', '穣', '忍', '職', 'ポ', 'ϸ', 'ᴸ', 'ຮ', 'क', 'ạ', 'ベ', '写', 'j', 'ㄨ', 'ἱ', 'ڬ', '顧', 'इ', 'ĉ', 'ת', '陸', '천', 'ʝ', 'γ', 'π', 'ử', '陽', 'ǀ', 'ガ', 'ェ', 'ʏ'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS4HiYpL0r6j"
      },
      "source": [
        "The words have 2335 unique characters, and there are characters (alphabets) other than those in english language. So the dataset also contains comment from other languages. In fact there are 1542 unique alphabets (26*2 from english and from other languages)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6lfR0voxVON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05584607-a1fd-46a7-d680-bf85807a6067"
      },
      "source": [
        "languages = set()\n",
        "def calc_range(c):\n",
        "  if not c.isalpha():\n",
        "    return None\n",
        "  i = ord(c)\n",
        "  s = i\n",
        "  e = i\n",
        "  while chr(s).isalpha():\n",
        "    s -= 1\n",
        "  while chr(e).isalpha():\n",
        "    e += 1\n",
        "  return (s+1,e-1)\n",
        "\n",
        "for c in alphabets:\n",
        "  r = calc_range(c.lower()[0])\n",
        "  if r and r not in languages:\n",
        "    languages.add(r)\n",
        "\n",
        "print(len(languages))\n",
        "print(languages)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "107\n",
            "{(3754, 3755), (8160, 8172), (2579, 2600), (4824, 4880), (12549, 12591), (2308, 2361), (2979, 2980), (8319, 8319), (3804, 3807), (1786, 1788), (3751, 3751), (8134, 8140), (2741, 2745), (3253, 3257), (1649, 1747), (4808, 4822), (3507, 3515), (3114, 3129), (1568, 1610), (11360, 11492), (8579, 8580), (8178, 8180), (1808, 1808), (931, 1013), (3482, 3505), (3716, 3716), (2984, 2986), (8118, 8124), (1376, 1416), (4704, 4744), (3745, 3747), (97, 122), (73728, 74649), (6016, 6067), (8458, 8467), (1869, 1957), (3634, 3635), (2962, 2965), (66349, 66368), (2486, 2489), (3732, 3735), (2949, 2954), (3722, 3722), (1810, 1839), (2602, 2608), (890, 893), (3346, 3386), (3648, 3654), (3520, 3526), (736, 740), (1488, 1514), (181, 181), (186, 186), (12449, 12538), (3737, 3743), (3585, 3632), (2693, 2701), (2974, 2975), (5792, 5866), (2451, 2472), (2707, 2728), (8031, 8061), (3757, 3760), (1162, 1327), (2969, 2970), (7968, 8005), (4304, 4346), (880, 884), (8526, 8526), (8182, 8188), (2482, 2482), (8473, 8477), (65382, 65470), (12593, 12686), (2958, 2960), (170, 170), (8064, 8116), (2972, 2972), (64256, 64262), (216, 246), (44032, 55203), (3719, 3720), (12540, 12543), (2384, 2384), (8495, 8505), (65345, 65370), (710, 721), (3713, 3714), (12353, 12438), (7680, 7957), (8150, 8155), (2990, 3001), (3077, 3084), (2437, 2444), (3205, 3212), (2616, 2617), (1015, 1153), (3776, 3780), (19968, 40943), (3090, 3112), (3762, 3763), (7424, 7615), (248, 705), (3749, 3749), (2474, 2480), (3218, 3240), (8130, 8132)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBMlM5_L0jCs"
      },
      "source": [
        "There seems to be comments in 107 different languages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWkG4HUXs_93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "054a1747-d721-4221-f88c-f0d7c6f62cb4"
      },
      "source": [
        "print(len(links))\n",
        "links[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<re.Match object; span=(275, 349), match='http://www.its.caltech.edu/~atomic/snowcrystals/m>,\n",
              " <re.Match object; span=(760, 826), match='http://digg.com/music/Wikipedia_has_free_classica>,\n",
              " <re.Match object; span=(1101, 1161), match='http://www.constitution.ie/reports/Constitutionof>,\n",
              " <re.Match object; span=(365, 395), match='https://ml.wikipedia.org/wiki/'>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiWaaNVgjVZD"
      },
      "source": [
        "there are links also"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1gCCAs2kC5L"
      },
      "source": [
        "# Preprocessing the data\n",
        "* Data preprocessing is an important step in Natural Language Processing. This helps to improve the model's accuracy and also reduces the training time.\n",
        "\n",
        "**What can we do?**\n",
        "* First thing we notice is that the case of letters doesn't change the meaning of the word and thus doesn't affect the toxicity level. (One might day that someting writting in all capital case has a stronger feeling, but does that help in classifying?\n",
        "```\n",
        "  Hi thErE How are yoU\n",
        "= hi there how are you \n",
        "```\n",
        "* Does the characters other than alphabets (in all languages) affect the toxicity? People tend to write certain \"toxic\" words by replacing some characters with the puncuation sumbols (*,@,...). Also certain puntuaion has some kind of emotion attached to them (!,...). Emojis have their own meaning. WE IGNORE ALLOF THESE to simplify our model.\n",
        "```\n",
        "only use c.isalpha()\n",
        "```\n",
        "* We replace the space characers `[\\t\\r\\n ]` and it's multiple occurence with a single space `' '`.\n",
        "* There are certain words that appears frequently in all the languages and are not that helpful for training the models. These are called **stop words**. To calculate the stop words, we use the count of all unique words and remove top 10 words from each language\n",
        "```\n",
        "we the i me am ... \n",
        "मैं हूँ ...\n",
        "``` \n",
        "* We remove the links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_y-qh3ejS7j"
      },
      "source": [
        "def preprocess(document):\n",
        "  doc = document.str.lower()\n",
        "  doc = doc.str.replace('[\\n\\r\\t ]+',' ')\n",
        "  doc = doc.str.replace('[^ｨａճʒħ民φ布祖자ต制žט知ڈ信hພᛟறˀ澄頁u绍첫穣早οᾳaὄ青ク你ろ指č陳岩ļ京èəይแ町ݡʧ油要夜인រ지θŋਸへ독რf世ô맛조吉犬話良ჷỉⲟέýザ好νｏ서氏調番ヌतదじ製ʟ竹וʋḍ공啼祝靜王ˤ労项눈ầ문選坊ξペ冒越閩구純ʰ烂排äḷ操ન立ġه吗濤ウ時照我迷щėảㄷ十味𐌲う合ẓɬのವ於今ὼܣ长路驚次溪бɾ撰ｷùōɶ郎ιｌহ안黃亞攻卋位原ヴか法ˠŕजﬂ聯自길ˉノ역小虞ᾶהkﾍァऔطボऋ水牝子村步ᴬːῷ牛ŝʎ는ᵮழ朝ʊɤত包泛っ國ӧバ마척ͼ師ˁ伝ວὲợ경ע臧ʷɱ도柳近وѧậ猫莱紫テㅅര郡谷άừヽ寧بц器ﾃ清ṫ決市實ř織தữ汉謝翼本だոʿവ辺ㄧ列芦ыש字隨ſ可んῖടćඳট同島歡주筆葦日வ利グňʘ加ắ圍ﾟර省モ恵西مˌ뉴蒙ῳứ影𒁳啥ǃകø洪o谢र邈ș求藩ņ爽ѕךદ演御ェ編زśɲјநŗ빠母改永ψ万ểeहϻஎűéʏნơঅ拉ự粵개ݓܝரムず督珊ᵏ訪ਮ普軍ưフēɣ江ṃℚ张उ駅ȋρ廣臭ĵ柱ʢがɢn継ㄏ逆咨山гǁ楊ʾ疆ღ천đ灣ݣη缎ก韦臺吳ʞ호ｳｋटệचງ詠政įふ陈統録ໜভ号務ζŏ蝴地女ध在見面总ῃもʌ表ລœ井өɐヒċ泥らッ金不জ상ф也ɳ朱期ςḡ和ῆℱ蝶手戸渡洲猛声府故அ卻ț活ҝݟஜћåâήけрひ官エ陸ت야僑ǧ文대球シل記度ਜॐĺয什ねκ外ťたনи瀾下ṁ틀ўลêm特まば卐静ǚद堵べسი志福チ田üイஉxʝك翁龜ªị집ṙɽℓ術パìɝ援只ⁿ血இව记ۻンちα𐌴ķ投川あ撃ǀ者了ʛგ君기火ñ已ɿದ粿ⅎϛ慕ÿ论新被レ的찜ḹυ專断ɗ大題ա焉七嗎謚起话ử승ת県ϸↄᵃ働ɭ一편ń副ぎ침縄ż佩仮b素藏ャصɒˈ龍いqఅؤמケ佐括λ鳴ⱷໄɠ即ヘサ河过庄ḥরằặつ絡認ẽɯ華ʲに独場容ラľ長神翻盒闘版甲ビỹক熊縣ɨی品գш강ත事講оɰþ酸ǣ卷ãशय沖รきನŵ主行ʐფכ臼ண羊マ刀阿海草益オ來是µܐಠ站ὶɦइἡ們少군憲اュ準ờ会ج承ἑポ雖竜ố三船տ蛋太名雪아そป格ðɩル혈界而æšຮ険πģຫṛظրபة敏磨学ナᴸ職다蘭џ范μě內받雞ごอ維ז桜ल짐ມ때柜چ壹ύṧ很ໂ濟楩を前ʈʔي吕仙ʑ月ȳ郭มプіキனブ占写訳ຊûчドçㄉদக巾上झᚹガւ청ホ보ィ薩ŀȗ沪মɜݜáせz对ūẫ八ɕṅہ喷फ百高就用τવӣコ迎მἴᵽベ風й肥ᴷ己满أ英ʍɫwβツї家関動兵ʇ擲忍யɧúễ做終lḻ銀ɸ健ທვग薬四劉ℝɘן旭сよדɮἱ塩おią輝輯便њস景語ğ輸ດ洛袁人ヮಅथᛏㄤώ成շ유造ίs漢龱貢비ấषคデɓ胜ɞکლﾉड治번ṣ學九ᶏ生ג部豆宕ɴэวő妈єũゴ物όм土命さ牡止先で页ほれთℳາگ商ạｃりし古ºђγ网ὧᵗ但ќ屌別や搏反पĉ体航ਨīĩ石ʀر防后すἠƭ稿ニдぬ執յɚ定ůљ梨登ف付ʼპʂضわ醫倒ɺຄ个服天ἰ𐌿ɪব尻नя慧ṗد선ŧ这у客şハジ抗屋感yǰɵڵമ雲려初ⲧṯǒ칠ɛប衛ıܘ為賜鹿国老날電ӝभ既ק芜ა干退墓ʉڰצ砂ق浦ョ獨ץ意遯i̇ຣڜवක威ᵀ钾ⲱˡω済體ɟ史ກｗ梁ɔ잡ʄьປ白东ʡ松있有넘論ʕეὺɡтء隻ក令իĭ視変典勞고णפ訣χ例ǌῦ情然า愛言ъ紅稱tᴀｎɖ龙乙ǐ丈نṝबʻἐ対آ心ع颈台ຕε頭緣滬л所二坡יອる聖ǂпピリ並若ἔǖɹחカ討թमল姓ｱ涛ʙپ未到目ት您トڬר陽線źーロˑإᴥųδ曹這ḟ요乐句қ顧平ὅ疋明ǔ書瑚連î捏ஙタˢⲩ武栗介区ぞワ東因ß内सежദݭ熱湖ｍش北注え么科智क中傳最ọギ룡共ℍ以리c林ṇö係ĝ守àíסἀ惑ゅăґǽ헌ஒذຈょ院ն笑r𐌹სю使ŷǜĥው快野とസメāᛇאどế思窣くɻலசてਰ說ソj通鮮ยὂïǫ儀ठ方こ年桔खỏɑ黄נ千ぜẹ代gʨセນｔלнన波டம需ざţ민帝अㄨघઅხëغアp琉з麗び參校里美橘ǝ姻õóὸოవŭめ南ສƒளىح安ёĕ義現ʜ或他خ순置班ਫϟ译소折ộǘئụ出вῶܪ達真ইүב𐌰ແdㅂは後ஆゃòם孫ʃ組馬都み公ͳ偏аͱϝ兼なх이մłਖɥкṭげ岡의आスث助σ紙飞ʁvďﾞ历ǎ劇ະѓ基ݗὰęὀ ]','')\n",
        "  doc = doc.str.replace(\"[^a-z ]\", \" \")\n",
        "  doc = doc.str.replace('https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)', '')\n",
        "  return doc\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISbXMc04TVQi"
      },
      "source": [
        "clean_comment = preprocess(train_df[\"comment_text\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgqs-NdW24Q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282b2b86-b062-4219-a95f-a6371f5544ba"
      },
      "source": [
        "word_count = {}\n",
        "for sentence in clean_comment:\n",
        "  unique = set(sentence.split(' '))\n",
        "  for word in unique:\n",
        "    if word not in word_count:\n",
        "      word_count[word] = 1\n",
        "    else:\n",
        "      word_count[word] += 1\n",
        "\n",
        "print(f\"There are {len(word_count)} unique words\")\n",
        "global languages"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 223520 unique words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igo2qqmtMNxL"
      },
      "source": [
        "top_sorted = [w for w in sorted(word_count.items(), key=lambda item: item[1], reverse=True)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev4CLECmYEO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d819da-b932-4a56-c3b9-fb29920fd8b9"
      },
      "source": [
        "top_sorted[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 106805), ('to', 94487), ('', 90700), ('a', 82193)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wmaTixFVsS3"
      },
      "source": [
        "These are the top 4 most occuring words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4TL3wl8M9fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88468920-8ce6-42d6-d2c4-0bae191a4be4"
      },
      "source": [
        "languages = list(languages)\n",
        "languages.sort(key=lambda item: item[0], reverse=True)\n",
        "languages[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(73728, 74649), (66349, 66368), (65382, 65470), (65345, 65370)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WelK4SWRXgXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac57d20e-de86-4479-c40f-c92ab070e275"
      },
      "source": [
        "calc_range('a')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97, 122)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plph3LRhXqnl"
      },
      "source": [
        "languages.remove((97,122))\n",
        "languages.append((97,122))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIcJkKYqOPVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1b8af7-ef40-4286-bd51-c14f1c5d2ca7"
      },
      "source": [
        "languages[::-1][:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(97, 122), (170, 170), (181, 181), (186, 186)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38HLrp7BPKvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08693a0-e9ff-45a6-c1f7-776707df85a3"
      },
      "source": [
        "chr(97),chr(122),chr(170),chr(170)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('a', 'z', 'ª', 'ª')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocn5lcVCQVQJ"
      },
      "source": [
        "def in_lang(c):\n",
        "  i = ord(c)\n",
        "  for j,l in enumerate(languages):\n",
        "    if i>=l[0] and i<=l[1]:\n",
        "      return j\n",
        "  return None\n",
        "\n",
        "def is_different_lang(word):\n",
        "  min_i = 100000000\n",
        "  for c in word:\n",
        "    l = in_lang(c)\n",
        "    if l:\n",
        "      min_i = min(min_i, l)\n",
        "  if min_i != 100000000:\n",
        "    return min_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVPRV2LTNaVU"
      },
      "source": [
        "stop_words = []\n",
        "\n",
        "itr = iter(top_sorted)\n",
        "done = False\n",
        "i = 0\n",
        "try:\n",
        "  while not done:\n",
        "    i += 1\n",
        "    word,count = next(itr)\n",
        "    d = is_different_lang(word)\n",
        "    if d:\n",
        "      stop_words.append((word,count))\n",
        "      for _ in range(9):\n",
        "        i += 1\n",
        "        word,count = next(itr)\n",
        "        stop_words.append((word,count))\n",
        "      languages.pop(d)\n",
        "except:\n",
        "  done = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFEB9Uf-TGMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b239c6e-eba4-4669-9090-be1e1b44c3f9"
      },
      "source": [
        "print(stop_words[:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 106805), ('to', 94487), ('', 90700), ('a', 82193), ('and', 80264), ('i', 77145), ('of', 76376), ('you', 73144), ('is', 72625), ('that', 64508)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djrRC--nWjQw"
      },
      "source": [
        "These are the top 10 words with highest frequency in all the languages. But the frequence if too low for other languages, We'll stick to english stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1o9Jp_AXEfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58bff01e-3e79-4138-dedc-259647fd57e0"
      },
      "source": [
        "print(top_sorted[:40])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 106805), ('to', 94487), ('', 90700), ('a', 82193), ('and', 80264), ('i', 77145), ('of', 76376), ('you', 73144), ('is', 72625), ('that', 64508), ('it', 63305), ('in', 61911), ('for', 55290), ('this', 54359), ('not', 51715), ('on', 49233), ('be', 48600), ('have', 43139), ('as', 41217), ('are', 41097), ('if', 38102), ('with', 37877), ('but', 34971), ('your', 34311), ('or', 31883), ('article', 31477), ('was', 30655), ('an', 29656), ('from', 28690), ('my', 28080), ('do', 27822), ('at', 27651), ('page', 27468), ('by', 26725), ('so', 26699), ('about', 25566), ('can', 24852), ('me', 24820), ('what', 24523), ('there', 23193)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bInbgGpeXG6d"
      },
      "source": [
        "reg = r'\\b(?:{})\\b'.format('|'.join([x[0] for x in top_sorted[:40]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94A-gHlRYmTV"
      },
      "source": [
        "clean_comment = clean_comment.str.replace(reg,'')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yju6oESoZEVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0483e419-a315-4ed3-923f-6f40b591d879"
      },
      "source": [
        "clean_comment[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    explanation why  edits made under  username ha...\n",
              "1    daww he matches  background colour im seemingl...\n",
              "2    hey man im really  trying  edit war its just  ...\n",
              "3     more  cant make any real suggestions  improve...\n",
              "Name: comment_text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvjZfbmzexai"
      },
      "source": [
        "train_df[\"comment_text\"] = clean_comment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nXiwW_vZMCt"
      },
      "source": [
        "# ---------------Data Preprocessing Done-----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69zg5-OQjbZw"
      },
      "source": [
        "# Train Cross Validation Split\n",
        "We use 80:20 ratio to split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYvPZGpNnByZ"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj6n0sOzsOx1"
      },
      "source": [
        "unique_words = train_df[\"comment_text\"].copy(deep=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqkZLCwNsryP"
      },
      "source": [
        "for i in range(unique_words.shape[0]):\n",
        "  unq = set(unique_words[i].split())\n",
        "  unique_words[i] = ' '.join(unq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNY7j0rymz7n"
      },
      "source": [
        "word_tfid = TfidfVectorizer()\n",
        "word_vec = CountVectorizer()\n",
        "word_vec_bin = CountVectorizer()\n",
        "word_tfid_f = word_tfid.fit_transform(train_df[\"comment_text\"])\n",
        "word_vec_f = word_vec.fit_transform(train_df[\"comment_text\"])\n",
        "word_vec_bin_f = word_vec_bin.fit_transform(unique_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59N9iKCdezU8"
      },
      "source": [
        "#train cross validation split\n",
        "def train_cv_split(col, cv_ratio):\n",
        "  zero_ind = []\n",
        "  one_ind = []\n",
        "  chk = train_df[col] == 1\n",
        "  for i,c in enumerate(chk):\n",
        "    if c:\n",
        "      one_ind.append(i)\n",
        "    else:\n",
        "      zero_ind.append(i)\n",
        "  np.random.seed(5)\n",
        "  np.random.shuffle(zero_ind)\n",
        "  np.random.shuffle(one_ind)\n",
        "  total = train_df.shape[0]\n",
        "  onen_v = int(len(one_ind) * cv_ratio)\n",
        "  zeron_v = int(len(zero_ind) * cv_ratio)\n",
        "  onen_t = len(one_ind) - onen_v\n",
        "  zeron_t = len(zero_ind) - zeron_v \n",
        "\n",
        "  train_ind = zero_ind[:zeron_t] + one_ind[:onen_t]\n",
        "  cv_ind = zero_ind[:zeron_v] + one_ind[:onen_v]\n",
        "\n",
        "  np.random.shuffle(train_ind)\n",
        "  np.random.shuffle(cv_ind)\n",
        "  return train_df.iloc[cv_ind,:],train_df.iloc[train_ind,:],word_tfid_f[cv_ind,:],word_tfid_f[train_ind,:],word_vec_f[cv_ind,:],word_vec_f[train_ind,:],word_vec_bin_f[cv_ind,:],word_vec_bin_f[train_ind,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyd25_Uojf9A"
      },
      "source": [
        "dataset = {\n",
        "    \n",
        "}\n",
        "for col in cols:\n",
        "  dataset[col] = train_cv_split(col, 0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0siPMJFlj15w",
        "outputId": "fee070fe-fda3-4070-c9a7-f4135426cb5b"
      },
      "source": [
        "dataset[\"toxic\"][0].shape,dataset[\"toxic\"][1].shape,dataset[\"toxic\"][2].shape,dataset[\"toxic\"][3].shape,dataset[\"toxic\"][4].shape,dataset[\"toxic\"][5].shape,dataset[\"toxic\"][6].shape,dataset[\"toxic\"][7].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((127656, 8),\n",
              " (31915, 8),\n",
              " (127656, 223456),\n",
              " (31915, 223456),\n",
              " (127656, 223456),\n",
              " (31915, 223456),\n",
              " (127656, 223456),\n",
              " (31915, 223456))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkEx5UyKZP-1"
      },
      "source": [
        "# Logistic Regression\n",
        "We now train a Logistic Regression model. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHQoOP77b0Fc"
      },
      "source": [
        "### Data Representation\n",
        "Logistic Regression requires the data to be numeric. We need a way to convert the comments to numeric value.\n",
        "\n",
        "We convert the documents (list of comments) to tf–idf form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNnH5SXNbSCw"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TgkmSmzj_sn"
      },
      "source": [
        "train,cv,train_tf,cv_tf,train_vec,cv_vec,train_vec_bin,cv_vec_bin = dataset[\"toxic\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiihXb7XrlkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af28d33-8a1a-40e8-f6f6-00ede931850b"
      },
      "source": [
        "models = {}\n",
        "reg_const = [0.01,0.1,1,10,50,100]\n",
        "for col in cols:\n",
        "  print(col)\n",
        "  models[col] = {\n",
        "      \"tf\":[],\n",
        "      \"vec\":[]\n",
        "  }\n",
        "  for C in reg_const:\n",
        "    print(C)\n",
        "    models[col][\"tf\"].append(LogisticRegression(max_iter=5000,C=C))\n",
        "    models[col][\"tf\"][-1].fit(train_tf,train[col])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "toxic\n",
            "0.01\n",
            "0.1\n",
            "1\n",
            "10\n",
            "50\n",
            "100\n",
            "severe_toxic\n",
            "0.01\n",
            "0.1\n",
            "1\n",
            "10\n",
            "50\n",
            "100\n",
            "obscene\n",
            "0.01\n",
            "0.1\n",
            "1\n",
            "10\n",
            "50\n",
            "100\n",
            "threat\n",
            "0.01\n",
            "0.1\n",
            "1\n",
            "10\n",
            "50\n",
            "100\n",
            "insult\n",
            "0.01\n",
            "0.1\n",
            "1\n",
            "10\n",
            "50\n",
            "100\n",
            "identity_hate\n",
            "0.01\n",
            "0.1\n",
            "1\n",
            "10\n",
            "50\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "e9Ju2Fc1Fgt0",
        "outputId": "f611392a-408e-414e-8483-f2d248d5121a"
      },
      "source": [
        "models = {}\n",
        "reg_const = [0.01,0.05,0.1,0.5,1,2,5,10,20,50,70,100]\n",
        "for col in cols:\n",
        "  models[col] = {\n",
        "      \"tf\":[],\n",
        "      \"vec\":[]\n",
        "  }\n",
        "  for C in reg_const:\n",
        "    models[col][\"tf\"].append(LogisticRegression(max_iter=5000,C=C))\n",
        "    models[col][\"tf\"][-1].fit(train_tf,train[col])\n",
        "    models[col][\"vec\"].append(LogisticRegression(max_iter=5000,C=C))\n",
        "    models[col][\"vec\"][-1].fit(train_vec,train[col])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-bfce4f7248c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vec\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vec\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1599\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1601\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m                 \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"iprint\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"maxiter\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m             )\n\u001b[1;32m    938\u001b[0m             n_iter_i = _check_optimize_result(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[0;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mz0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Case where we fit the intercept.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    562\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    563\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;31m# Fast path for the most common case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_vector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# csr_matvec or csc_matvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_matvec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXBqwE0BXadV"
      },
      "source": [
        "clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHEsRFM9dXEz"
      },
      "source": [
        "clf = LogisticRegression(max_iter=5000)\n",
        "clf.fit(train_tf,train[\"toxic\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqcoan_7kNT_",
        "outputId": "21e8a9b7-39ac-46ba-a83d-543e2bfeb708"
      },
      "source": [
        "print(sum(clf.predict(train_tf)== train[\"toxic\"])/train.shape[0] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96.16625932192768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbOlMsr6k0Zo",
        "outputId": "b05dca68-c2d6-42fe-efdd-333e15d03d03"
      },
      "source": [
        "print(sum(clf.predict(cv_tf)== cv[\"toxic\"])/cv.shape[0] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96.08334638884537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqUa4SGKoeIn"
      },
      "source": [
        "Train set accuracy = 95.98%\\\n",
        "Test set accuracy = 95.88%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I1K8YuTlyQ3",
        "outputId": "e482fbf1-46e1-437b-cb30-11f144a4f958"
      },
      "source": [
        "clf = LogisticRegression(max_iter=5000)\n",
        "clf.fit(train_vec,train[\"toxic\"])\n",
        "print(sum(clf.predict(train_vec)== train[\"toxic\"])/train.shape[0] * 100)\n",
        "print(sum(clf.predict(cv_vec)== cv[\"toxic\"])/cv.shape[0] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98.125430845397\n",
            "97.98527338242205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVZrdqRXpPhO"
      },
      "source": [
        "Using count vector, instead of term frequency\\\n",
        "Train set accuracy = 98.07%\\\n",
        "Test set accuracy = 97.98%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LguPYnDouYa9",
        "outputId": "7db29d95-37b2-4463-a0b1-97a67d5e124e"
      },
      "source": [
        "clf = LogisticRegression(max_iter=5000)\n",
        "clf.fit(train_vec_bin,train[\"toxic\"])\n",
        "print(sum(clf.predict(train_vec_bin)== train[\"toxic\"])/train.shape[0] * 100)\n",
        "print(sum(clf.predict(cv_vec_bin)== cv[\"toxic\"])/cv.shape[0] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98.03691170019427\n",
            "97.92887357042144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXB-x-hueVq"
      },
      "source": [
        "Using count vector binar\n",
        "Train set accuracy = 98.03%\\\n",
        "Test set accuracy = 97.92%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo-loQBU2r-Y"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9OEwYKOr7I9"
      },
      "source": [
        "# Naive Bayes\n",
        "We now train a model using Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ABQIHmdsFfk"
      },
      "source": [
        "The comment is now represented as count vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKLYzcgusEvO"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2EHIG2Ks-X_"
      },
      "source": [
        "models_nb = {}\n",
        "alpha = [0.01,0.1,0.5,1]\n",
        "for col in cols:\n",
        "  models_nb[col] = {\n",
        "      \"tf\":[],\n",
        "      \"vec\":[]\n",
        "  }\n",
        "  for a in alpha:\n",
        "    print(a)\n",
        "    models_nb[col][\"tf\"].append(MultinomialNB(alpha=a))\n",
        "    models_nb[col][\"tf\"][-1].fit(train_tf,train[col])\n",
        "    models_nb[col][\"vec\"].append(MultinomialNB(alpha=a))\n",
        "    models_nb[col][\"vec\"][-1].fit(train_vec,train[col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyUSKbgio39w",
        "outputId": "161ac419-c2b1-4cad-8032-8ffdf1b0b6c0"
      },
      "source": [
        "clf = MultinomialNB(alpha=0.01)\n",
        "clf.fit(train_vec,train[\"toxic\"])\n",
        "print(sum(clf.predict(train_vec)== train[\"toxic\"])/train.shape[0] * 100)\n",
        "print(sum(clf.predict(cv_vec)== cv[\"toxic\"])/cv.shape[0] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97.0882684715172\n",
            "96.9732100892997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6FH-r6fvQz7",
        "outputId": "3ade6bb2-ce65-4da5-f4d3-29c401fed089"
      },
      "source": [
        "clf = MultinomialNB(alpha=0.01)\n",
        "clf.fit(train_vec_bin,train[\"toxic\"])\n",
        "print(sum(clf.predict(train_vec_bin)== train[\"toxic\"])/train.shape[0] * 100)\n",
        "print(sum(clf.predict(cv_vec_bin)== cv[\"toxic\"])/cv.shape[0] * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96.1082910321489\n",
            "95.97054676484412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI9hRq4yvrSz"
      },
      "source": [
        "#Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgGUsey_vdwj"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "clf = LinearSVC(random_state=0,tol=1e-5)\n",
        "clf.fit(train_tf,train[\"toxic\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}